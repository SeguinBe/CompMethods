{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Efficient structure for saving lots of parsed documents\n",
    "doc_bin = DocBin(attrs=[\"HEAD\", \"TAG\", \"LEMMA\", \"DEP\", \"POS\"], store_user_data=True)\n",
    "\n",
    "# Reading the binary file, 558 pre-parsed texts (376MB)\n",
    "with open('texts_english.bin', 'rb') as f:\n",
    "    loaded_data = doc_bin.from_bytes(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: loading all the 558 books takes roughly 6GB of RAM\n",
    "# You might want to try on a small subset first\n",
    "N_BOOKS = 30\n",
    "\n",
    "docs = []\n",
    "N_BOOKS = min(N_BOOKS, len(loaded_data))\n",
    "for i, doc in zip(tqdm(range(N_BOOKS)), loaded_data.get_docs(nlp.vocab)):\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each book is just a large Spacy Doc, with the information of the corresponding original text file\n",
    "docs[0].user_data['file']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making it work on examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = nlp('What is Beauty?')\n",
    "displacy.render(d)\n",
    "for t in d:\n",
    "    print(t, t.lemma_, t.pos_, t.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some example sentences that can be useful for trying out your rule matching algorithm.\n",
    "\n",
    "Do not be afraid of exploring the xenotheka data with simple rules (every appearance of the word beauty for instance) to find examples that could extend these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_sentences = [\n",
    "    'What is beauty?',\n",
    "    'Beauty is something',\n",
    "    'Beauty can be defined',\n",
    "    'In my opinion, beauty is'\n",
    "]\n",
    "wrong_sentences = [\n",
    "    \"Athena's beauty\",\n",
    "    \"The beauty of this approach lies in\",\n",
    "    \"It is one of many beauties\",\n",
    "    \"What strikes the viewer is the beauty of the place\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining your rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "# Create a matcher object\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "matcher.add(\"isDefiningBeauty\", None, [{\"LEMMA\": \"beauty\"}])\n",
    "\n",
    "def is_token_defining_beauty(token):\n",
    "    # Remember that a token has lots of info (token.lemma_, token.tag_, token.pos_)\n",
    "    # And that one can access the neighbouring tokens, or the dependency tree (token.head, token.children)\n",
    "    # The full sentence is also accessible (token.sent)\n",
    "    \n",
    "    # Dummy condition as an example, the sentence containing the token has to be have at least 2 tokens\n",
    "    if len(token.sent) < 2:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "# You might want to try different approaches\n",
    "def is_defining_beauty_version_1(token):\n",
    "    # the direct ancestor of the token is a verb\n",
    "    if token.head.pos_ == 'VERB':\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying it on examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code tries the rule you have created on the examples and tell you\n",
    "# if valid sentences were missed or if invalid ones were matched\n",
    "\n",
    "for txt in valid_sentences:\n",
    "    d = nlp(txt)\n",
    "    matches = matcher(d)\n",
    "    found_it = False\n",
    "    for match_id, start, end in matches:\n",
    "        # Get the token from the starting position in the document\n",
    "        t = d[start]\n",
    "        # Add additional logic here\n",
    "        if is_token_defining_beauty(t):\n",
    "            found_it = True\n",
    "    if not found_it:\n",
    "        print('MISSING VALID')\n",
    "        print(txt)\n",
    "            \n",
    "for txt in wrong_sentences:\n",
    "    d = nlp(txt)\n",
    "    matches = matcher(d)\n",
    "    found_it = False\n",
    "    for match_id, start, end in matches:\n",
    "        # Get the token from the starting position in the document\n",
    "        t = d[start]\n",
    "        # Add additional logic here\n",
    "        if is_token_defining_beauty(t):\n",
    "            found_it = True\n",
    "    if found_it:\n",
    "        print('INVALID')\n",
    "        print(t.sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying on the Xenotheka data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in tqdm(docs):\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        # match_id is the id of the matching rule, this line is to get back the name 'myVeryOwnMacthingRule'\n",
    "        string_id = nlp.vocab.strings[match_id]\n",
    "        # Get the token from the starting position in the document\n",
    "        t = doc[start]\n",
    "        if is_token_defining_beauty(t):\n",
    "            filename = doc.user_data['file']\n",
    "            # Printing the original filename and the approximate position in the document\n",
    "            print(f'{filename} (token {t.i}/{len(doc)} - {t.i*100/len(doc):f}%)')\n",
    "            print(t.sent)\n",
    "            print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
